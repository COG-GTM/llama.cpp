
model:
  path: "models/my-model.gguf"

n_predict: 128
n_ctx: 4096
n_batch: 512
n_ubatch: 512
n_keep: 0
n_gpu_layers: 32
main_gpu: 0
verbosity: 1

prompt: "Hello, how are you?"
system_prompt: "You are a helpful assistant."
input_prefix: "User: "
input_suffix: "\nAssistant: "

rope_freq_base: 10000.0
rope_freq_scale: 1.0
yarn_ext_factor: 1.0
yarn_attn_factor: 1.0
yarn_beta_fast: 32.0
yarn_beta_slow: 1.0

interactive: false
interactive_first: false
conversation: false
use_color: true
simple_io: false
embedding: false
escape: true
multiline_input: false
cont_batching: true
flash_attn: false
no_perf: false
ctx_shift: true
input_prefix_bos: false
logits_all: false
use_mmap: true
use_mlock: false
verbose_prompt: false
display_prompt: true
dump_kv_cache: false
no_kv_offload: false
warmup: true
check_tensors: false

cache_type_k: "f16"
cache_type_v: "f16"

cpuparams:
  n_threads: 8
  strict_cpu: false
  poll: 50

sampling:
  seed: 42
  n_prev: 64
  n_probs: 0
  min_keep: 0
  top_k: 40
  top_p: 0.95
  min_p: 0.05
  xtc_probability: 0.0
  xtc_threshold: 0.1
  typ_p: 1.0
  temp: 0.8
  dynatemp_range: 0.0
  dynatemp_exponent: 1.0
  penalty_last_n: 64
  penalty_repeat: 1.0
  penalty_freq: 0.0
  penalty_present: 0.0
  dry_multiplier: 0.0
  dry_base: 1.75
  dry_allowed_length: 2
  dry_penalty_last_n: -1
  mirostat: 0
  top_n_sigma: 0.0
  mirostat_tau: 5.0
  mirostat_eta: 0.1
  ignore_eos: false
  no_perf: false
  timing_per_token: false
  grammar: ""
  grammar_lazy: false
  dry_sequence_breakers:
    - "\n"
    - ":"
    - "\""

speculative:
  n_ctx: 0
  n_max: 16
  n_min: 5
  n_gpu_layers: 0
  p_split: 0.1
  p_min: 0.9
  model:
    path: ""

antiprompt:
  - "User:"
  - "Human:"
  - "\n\n"
